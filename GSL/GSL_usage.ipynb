{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "DAgF15c514JP"
      ],
      "mount_file_id": "1Jo2wTp4dDEPa-8GOz91bJoJBdkyLhH2J",
      "authorship_tag": "ABX9TyNr2y78sDKkdlv8HcgAQaN8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/martintmv-git/RB-IBDM/blob/main/GSL/GSL_usage.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"align-center\">\n",
        "  <a href=\"https://www.fontys.nl/\"><img src=\"https://www.fontys.nl/static/design/FA845701-BD71-466E-9B3D-38580DFAD5B4-fsm/images/logo-inverted@2x.png\" height=\"75\"></a>\n",
        "  <img src=\"https://i.imgur.com/zyfbV3r.png\" width=\"20\">\n",
        "  <a href=\"https://www.has.nl/\"><img src=\"https://i.imgur.com/ZxkugVW.png\" height=\"75\"></a>\n",
        "  <img src=\"https://i.imgur.com/zyfbV3r.png\" width=\"20\">\n",
        "  <a href=\"https://www.has.nl/onderzoek/lectoraten/lectoraat-innovatieve-biomonitoring/\"><img src=\"https://i.imgur.com/oH3VJpE.png\" height=\"75\"></a>\n",
        "  <img src=\"https://i.imgur.com/zyfbV3r.png\" width=\"20\">\n",
        "  <a href=\"https://www.naturalis.nl/\"><img src=\"https://i.imgur.com/mAHW7XQ.png\" height=\"75\"></a>\n",
        "  <img src=\"https://i.imgur.com/zyfbV3r.png\" width=\"20\">\n",
        "  <a href=\"https://www.arise-biodiversity.nl/\"><img src=\"https://i.imgur.com/j6gBpqT.png\" height=\"75\"></a>\n",
        "  <img src=\"https://i.imgur.com/zyfbV3r.png\" width=\"20\">\n",
        "  <a href=\"https://faunabit.eu/\"><img src=\"https://i.imgur.com/HxqzRYg.png\" height=\"70\"></a>\n",
        "  <img src=\"https://i.imgur.com/zyfbV3r.png\" width=\"20\">\n",
        "  <a href=\"https://diopsis.eu/\"><img src=\"https://i.imgur.com/NHZ8e1b.png\" height=\"75\"></a>\n",
        "</div>\n",
        "\n",
        "# **How to Use the GSL Background Subtitution on New Images**\n",
        "\n",
        "---\n",
        "\n",
        "[![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/martintmv-git/RB-IBDM) <a href=\"https://github.com/facebookresearch/detectron2\"><img src=\"https://dl.fbaipublicfiles.com/detectron2/Detectron2-Logo-Horz.png\" width=\"120\"></a>\n",
        "\n",
        "<b>This notebook contains two main parts:\n",
        "1. Environment setup\n",
        "2. Usage\n",
        "\n",
        "You need to run the cells in the forst part in order to get everything set up and ready to use and then pass either the path to the image to process or the path to the folder containing multiple images to the `process` function for inference.</b>"
      ],
      "metadata": {
        "id": "bkPTNDJa1VcM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Environment setup"
      ],
      "metadata": {
        "id": "DAgF15c514JP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6kuyjy8E0T8x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2836ba6a-1289-408c-8c96-51c7b89bc37a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#GroundedSAM\n",
        "%cd /content\n",
        "!git clone https://github.com/IDEA-Research/Grounded-Segment-Anything\n",
        "%cd /content/Grounded-Segment-Anything\n",
        "!pip install -q -r requirements.txt\n",
        "%cd /content/Grounded-Segment-Anything/GroundingDINO\n",
        "!pip install -q .\n",
        "%cd /content/Grounded-Segment-Anything/segment_anything\n",
        "!pip install -q .\n",
        "%cd /content/Grounded-Segment-Anything\n",
        "\n",
        "#LaMa\n",
        "!pip install -q simple-lama-inpainting\n",
        "\n",
        "#EXIF\n",
        "!pip install -q piexif"
      ],
      "metadata": {
        "id": "eHqJDPUN2A4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, sys\n",
        "\n",
        "sys.path.append(os.path.join(os.getcwd(), \"GroundingDINO\"))\n",
        "\n",
        "import argparse\n",
        "import copy\n",
        "\n",
        "from IPython.display import display\n",
        "import PIL\n",
        "from PIL import Image, ImageDraw, ImageFont, ImageChops, ImageEnhance\n",
        "import piexif\n",
        "from torchvision.ops import box_convert\n",
        "\n",
        "# GroundingDINO\n",
        "import GroundingDINO.groundingdino.datasets.transforms as T\n",
        "from GroundingDINO.groundingdino.models import build_model\n",
        "from GroundingDINO.groundingdino.util import box_ops\n",
        "from GroundingDINO.groundingdino.util.slconfig import SLConfig\n",
        "from GroundingDINO.groundingdino.util.utils import clean_state_dict, get_phrases_from_posmap\n",
        "from GroundingDINO.groundingdino.util.inference import annotate, load_image, predict\n",
        "\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "import supervision as sv\n",
        "\n",
        "# SAM\n",
        "from segment_anything import build_sam, SamPredictor\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# LaMa\n",
        "import requests\n",
        "import torch\n",
        "from io import BytesIO\n",
        "from simple_lama_inpainting import SimpleLama"
      ],
      "metadata": {
        "id": "6XT008qC2Bzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "7zUXE4Iv2H3V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model_hf(repo_id, filename, ckpt_config_filename, device='cpu'):\n",
        "    cache_config_file = hf_hub_download(repo_id=repo_id, filename=ckpt_config_filename)\n",
        "\n",
        "    args = SLConfig.fromfile(cache_config_file)\n",
        "    args.device = device\n",
        "    model = build_model(args)\n",
        "\n",
        "    cache_file = hf_hub_download(repo_id=repo_id, filename=filename)\n",
        "    checkpoint = torch.load(cache_file, map_location=device)\n",
        "    log = model.load_state_dict(clean_state_dict(checkpoint['model']), strict=False)\n",
        "    print(\"Model loaded from {} \\n => {}\".format(cache_file, log))\n",
        "    _ = model.eval()\n",
        "    return model"
      ],
      "metadata": {
        "id": "V-a0jMrx2KIV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ckpt_repo_id = \"ShilongLiu/GroundingDINO\"\n",
        "ckpt_filenmae = \"groundingdino_swinb_cogcoor.pth\"\n",
        "ckpt_config_filename = \"GroundingDINO_SwinB.cfg.py\"\n",
        "\n",
        "\n",
        "groundingdino_model = load_model_hf(ckpt_repo_id, ckpt_filenmae, ckpt_config_filename, device)"
      ],
      "metadata": {
        "id": "Fq7OpyBX2K8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\n",
        "\n",
        "sam_checkpoint = 'sam_vit_h_4b8939.pth'\n",
        "\n",
        "sam_predictor = SamPredictor(build_sam(checkpoint=sam_checkpoint).to(device))"
      ],
      "metadata": {
        "id": "mA_2KzCl2NLV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "simple_lama = SimpleLama()"
      ],
      "metadata": {
        "id": "0DAiJ8YE2Pi5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def single_process(image_path):\n",
        "  path, file_name = os.path.split(image_path)\n",
        "  save_path = os.path.join(path, file_name.replace('.jpg', '_gsl.jpg'))\n",
        "\n",
        "  # Read image\n",
        "  image_source, image = load_image(image_path)\n",
        "\n",
        "  # detect insects using GroundingDINO\n",
        "  def detect(image, model, text_prompt = 'insect . flower . cloud', box_threshold = 0.25, text_threshold = 0.25):\n",
        "    boxes, logits, phrases = predict(\n",
        "        image=image,\n",
        "        model=model,\n",
        "        caption=text_prompt,\n",
        "        box_threshold=box_threshold,\n",
        "        text_threshold=text_threshold,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    annotated_frame = annotate(image_source=image_source, boxes=boxes, logits=logits, phrases=phrases)\n",
        "    annotated_frame = annotated_frame[...,::-1] # BGR to RGB\n",
        "    return annotated_frame, boxes, phrases\n",
        "\n",
        "  annotated_frame, detected_boxes, phrases = detect(image, model=groundingdino_model)\n",
        "\n",
        "  indices = [i for i, s in enumerate(phrases) if 'insect' in s]\n",
        "\n",
        "  def segment(image, sam_model, boxes):\n",
        "    sam_model.set_image(image)\n",
        "    H, W, _ = image.shape\n",
        "    boxes_xyxy = box_ops.box_cxcywh_to_xyxy(boxes) * torch.Tensor([W, H, W, H])\n",
        "\n",
        "    transformed_boxes = sam_model.transform.apply_boxes_torch(boxes_xyxy.to(device), image.shape[:2])\n",
        "    masks, _, _ = sam_model.predict_torch(\n",
        "        point_coords = None,\n",
        "        point_labels = None,\n",
        "        boxes = transformed_boxes,\n",
        "        multimask_output = True,\n",
        "        )\n",
        "    return masks.cpu()\n",
        "\n",
        "  def draw_mask(mask, image, random_color=True):\n",
        "      if random_color:\n",
        "          color = np.concatenate([np.random.random(3), np.array([0.8])], axis=0)\n",
        "      else:\n",
        "          color = np.array([30/255, 144/255, 255/255, 0.6])\n",
        "      h, w = mask.shape[-2:]\n",
        "      mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
        "\n",
        "      annotated_frame_pil = Image.fromarray(image).convert(\"RGBA\")\n",
        "      mask_image_pil = Image.fromarray((mask_image.cpu().numpy() * 255).astype(np.uint8)).convert(\"RGBA\")\n",
        "\n",
        "      return np.array(Image.alpha_composite(annotated_frame_pil, mask_image_pil))\n",
        "\n",
        "  segmented_frame_masks = segment(image_source, sam_predictor, boxes=detected_boxes[indices])\n",
        "\n",
        "  # combine all masks into one for easy visualization\n",
        "  final_mask = None\n",
        "  for i in range(len(segmented_frame_masks) - 1):\n",
        "    if final_mask is None:\n",
        "      final_mask = np.bitwise_or(segmented_frame_masks[i][0].cpu(), segmented_frame_masks[i+1][0].cpu())\n",
        "    else:\n",
        "      final_mask = np.bitwise_or(final_mask, segmented_frame_masks[i+1][0].cpu())\n",
        "\n",
        "  annotated_frame_with_mask = draw_mask(final_mask, image_source)\n",
        "\n",
        "  def dilate_mask(mask, dilate_factor=15):\n",
        "      mask = mask.astype(np.uint8)\n",
        "      mask = cv2.dilate(\n",
        "          mask,\n",
        "          np.ones((dilate_factor, dilate_factor), np.uint8),\n",
        "          iterations=1\n",
        "      )\n",
        "      return mask\n",
        "\n",
        "  # original image\n",
        "  image_source_pil = Image.fromarray(image_source)\n",
        "\n",
        "  # create mask image\n",
        "  mask = final_mask.numpy()\n",
        "  mask = mask.astype(np.uint8) * 255\n",
        "  image_mask_pil = Image.fromarray(mask)\n",
        "\n",
        "  # dilate mask\n",
        "  mask = dilate_mask(mask)\n",
        "  dilated_image_mask_pil = Image.fromarray(mask)\n",
        "\n",
        "  result = simple_lama(image_source, dilated_image_mask_pil)\n",
        "\n",
        "  img1 = Image.fromarray(image_source)\n",
        "  img2 = result\n",
        "\n",
        "  diff = ImageChops.difference(img2, img1)\n",
        "\n",
        "  threshold = 7\n",
        "  # Grayscale\n",
        "  diff2 = diff.convert('L')\n",
        "  # Threshold\n",
        "  diff2 = diff2.point( lambda p: 255 if p > threshold else 0 )\n",
        "  # # To mono\n",
        "  diff2 = diff2.convert('1')\n",
        "\n",
        "  img3 = Image.new('RGB', img1.size, (255, 236, 10))\n",
        "  diff3 = Image.composite(img1, img3, diff2)\n",
        "  diff3.save(save_path)\n",
        "  piexif.transplant(image_path, save_path)\n",
        "  diff3\n",
        "  print('Processing completed!')\n",
        "\n",
        "def batch_process(path):\n",
        "  save_path = os.path.join(path, 'GSL_output')\n",
        "  if os.path.exists(save_path) == False:\n",
        "    os.mkdir(save_path)\n",
        "\n",
        "  for file in os.listdir(path):\n",
        "    if file.endswith('.jpg'):\n",
        "      # Read image\n",
        "      image_source, image = load_image(os.path.join(path, file))\n",
        "\n",
        "      # detect insects using GroundingDINO\n",
        "      def detect(image, model, text_prompt = 'insect . flower . cloud', box_threshold = 0.25, text_threshold = 0.25):\n",
        "        boxes, logits, phrases = predict(\n",
        "            image=image,\n",
        "            model=model,\n",
        "            caption=text_prompt,\n",
        "            box_threshold=box_threshold,\n",
        "            text_threshold=text_threshold,\n",
        "            device=device\n",
        "        )\n",
        "\n",
        "        annotated_frame = annotate(image_source=image_source, boxes=boxes, logits=logits, phrases=phrases)\n",
        "        annotated_frame = annotated_frame[...,::-1] # BGR to RGB\n",
        "        return annotated_frame, boxes, phrases\n",
        "\n",
        "      annotated_frame, detected_boxes, phrases = detect(image, model=groundingdino_model)\n",
        "\n",
        "      indices = [i for i, s in enumerate(phrases) if 'insect' in s]\n",
        "\n",
        "      def segment(image, sam_model, boxes):\n",
        "        sam_model.set_image(image)\n",
        "        H, W, _ = image.shape\n",
        "        boxes_xyxy = box_ops.box_cxcywh_to_xyxy(boxes) * torch.Tensor([W, H, W, H])\n",
        "\n",
        "        transformed_boxes = sam_model.transform.apply_boxes_torch(boxes_xyxy.to(device), image.shape[:2])\n",
        "        masks, _, _ = sam_model.predict_torch(\n",
        "            point_coords = None,\n",
        "            point_labels = None,\n",
        "            boxes = transformed_boxes,\n",
        "            multimask_output = True,\n",
        "            )\n",
        "        return masks.cpu()\n",
        "\n",
        "      def draw_mask(mask, image, random_color=True):\n",
        "          if random_color:\n",
        "              color = np.concatenate([np.random.random(3), np.array([0.8])], axis=0)\n",
        "          else:\n",
        "              color = np.array([30/255, 144/255, 255/255, 0.6])\n",
        "          h, w = mask.shape[-2:]\n",
        "          mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
        "\n",
        "          annotated_frame_pil = Image.fromarray(image).convert(\"RGBA\")\n",
        "          mask_image_pil = Image.fromarray((mask_image.cpu().numpy() * 255).astype(np.uint8)).convert(\"RGBA\")\n",
        "\n",
        "          return np.array(Image.alpha_composite(annotated_frame_pil, mask_image_pil))\n",
        "\n",
        "      segmented_frame_masks = segment(image_source, sam_predictor, boxes=detected_boxes[indices])\n",
        "\n",
        "      # combine all masks into one for easy visualization\n",
        "      final_mask = None\n",
        "      for i in range(len(segmented_frame_masks) - 1):\n",
        "        if final_mask is None:\n",
        "          final_mask = np.bitwise_or(segmented_frame_masks[i][0].cpu(), segmented_frame_masks[i+1][0].cpu())\n",
        "        else:\n",
        "          final_mask = np.bitwise_or(final_mask, segmented_frame_masks[i+1][0].cpu())\n",
        "\n",
        "      annotated_frame_with_mask = draw_mask(final_mask, image_source)\n",
        "\n",
        "      def dilate_mask(mask, dilate_factor=15):\n",
        "          mask = mask.astype(np.uint8)\n",
        "          mask = cv2.dilate(\n",
        "              mask,\n",
        "              np.ones((dilate_factor, dilate_factor), np.uint8),\n",
        "              iterations=1\n",
        "          )\n",
        "          return mask\n",
        "\n",
        "      # original image\n",
        "      image_source_pil = Image.fromarray(image_source)\n",
        "\n",
        "      # create mask image\n",
        "      mask = final_mask.numpy()\n",
        "      mask = mask.astype(np.uint8) * 255\n",
        "      image_mask_pil = Image.fromarray(mask)\n",
        "\n",
        "      # dilate mask\n",
        "      mask = dilate_mask(mask)\n",
        "      dilated_image_mask_pil = Image.fromarray(mask)\n",
        "\n",
        "      result = simple_lama(image_source, dilated_image_mask_pil)\n",
        "\n",
        "      img1 = Image.fromarray(image_source)\n",
        "      img2 = result\n",
        "\n",
        "      diff = ImageChops.difference(img2, img1)\n",
        "\n",
        "      threshold = 7\n",
        "      # Grayscale\n",
        "      diff2 = diff.convert('L')\n",
        "      # Threshold\n",
        "      diff2 = diff2.point( lambda p: 255 if p > threshold else 0 )\n",
        "      # # To mono\n",
        "      diff2 = diff2.convert('1')\n",
        "\n",
        "      img3 = Image.new('RGB', img1.size, (255, 236, 10))\n",
        "      diff3 = Image.composite(img1, img3, diff2)\n",
        "      diff3.save(os.path.join(save_path, file))\n",
        "      piexif.transplant(os.path.join(path, file), os.path.join(save_path, file))\n",
        "  print('Batch completed, find processed images in GSL_output!')\n",
        "\n",
        "def process(path):\n",
        "  if os.path.isdir(path):\n",
        "    batch_process(path)\n",
        "  else:\n",
        "    single_process(path)"
      ],
      "metadata": {
        "id": "T7FZro-L2Sk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference"
      ],
      "metadata": {
        "id": "WfIRLxyl2bKy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Single image"
      ],
      "metadata": {
        "id": "6z3iHG-22cEb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select image file\n",
        "image_path = '/content/drive/MyDrive/Fontys/Fontys_Sem7/insect_detection/GSL/GSL_test_images/20230715012944.jpg'\n",
        "process(image_path)"
      ],
      "metadata": {
        "id": "ONG8xdqO2cfO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b279fa1f-a88d-43cb-e4cc-73aec0fe0a81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "SupervisionWarnings: annotate is deprecated: `BoxAnnotator` is deprecated and will be removed in `supervision-0.22.0`. Use `BoundingBoxAnnotator` and `LabelAnnotator` instead\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Batch"
      ],
      "metadata": {
        "id": "pYyKPzAQ2c2J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select folder containing images\n",
        "folder_path = '/content/drive/MyDrive/Fontys/Fontys_Sem7/insect_detection/GSL/GSL_test_images'\n",
        "process(folder_path)"
      ],
      "metadata": {
        "id": "FqpVt5Kb2dSE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64ba6473-27e1-490d-c6e0-ba7fdd281970"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SupervisionWarnings: annotate is deprecated: `BoxAnnotator` is deprecated and will be removed in `supervision-0.22.0`. Use `BoundingBoxAnnotator` and `LabelAnnotator` instead\n",
            "SupervisionWarnings: annotate is deprecated: `BoxAnnotator` is deprecated and will be removed in `supervision-0.22.0`. Use `BoundingBoxAnnotator` and `LabelAnnotator` instead\n",
            "SupervisionWarnings: annotate is deprecated: `BoxAnnotator` is deprecated and will be removed in `supervision-0.22.0`. Use `BoundingBoxAnnotator` and `LabelAnnotator` instead\n",
            "SupervisionWarnings: annotate is deprecated: `BoxAnnotator` is deprecated and will be removed in `supervision-0.22.0`. Use `BoundingBoxAnnotator` and `LabelAnnotator` instead\n",
            "SupervisionWarnings: annotate is deprecated: `BoxAnnotator` is deprecated and will be removed in `supervision-0.22.0`. Use `BoundingBoxAnnotator` and `LabelAnnotator` instead\n",
            "SupervisionWarnings: annotate is deprecated: `BoxAnnotator` is deprecated and will be removed in `supervision-0.22.0`. Use `BoundingBoxAnnotator` and `LabelAnnotator` instead\n",
            "SupervisionWarnings: annotate is deprecated: `BoxAnnotator` is deprecated and will be removed in `supervision-0.22.0`. Use `BoundingBoxAnnotator` and `LabelAnnotator` instead\n",
            "SupervisionWarnings: annotate is deprecated: `BoxAnnotator` is deprecated and will be removed in `supervision-0.22.0`. Use `BoundingBoxAnnotator` and `LabelAnnotator` instead\n",
            "SupervisionWarnings: annotate is deprecated: `BoxAnnotator` is deprecated and will be removed in `supervision-0.22.0`. Use `BoundingBoxAnnotator` and `LabelAnnotator` instead\n",
            "SupervisionWarnings: annotate is deprecated: `BoxAnnotator` is deprecated and will be removed in `supervision-0.22.0`. Use `BoundingBoxAnnotator` and `LabelAnnotator` instead\n",
            "SupervisionWarnings: annotate is deprecated: `BoxAnnotator` is deprecated and will be removed in `supervision-0.22.0`. Use `BoundingBoxAnnotator` and `LabelAnnotator` instead\n",
            "SupervisionWarnings: annotate is deprecated: `BoxAnnotator` is deprecated and will be removed in `supervision-0.22.0`. Use `BoundingBoxAnnotator` and `LabelAnnotator` instead\n",
            "SupervisionWarnings: annotate is deprecated: `BoxAnnotator` is deprecated and will be removed in `supervision-0.22.0`. Use `BoundingBoxAnnotator` and `LabelAnnotator` instead\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch completed, find processed images in GSL_output!\n"
          ]
        }
      ]
    }
  ]
}